{"cells":[{"cell_type":"markdown","source":["# Drive"],"metadata":{"id":"2m9NaivlpUHq"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zIy-v1CXBfCX","outputId":"51f0f018-f5f3-4797-a8e2-ad0244fc7f23","executionInfo":{"status":"ok","timestamp":1671282666732,"user_tz":-60,"elapsed":17527,"user":{"displayName":"Daniele_Casciani","userId":"18232816629247177598"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /gdrive\n"]}],"source":["from google.colab import drive\n","drive.mount('/gdrive')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UwfqICr1BxZ7","outputId":"fec4d62f-dd15-4ac3-c6c4-12be34c39b93","executionInfo":{"status":"ok","timestamp":1671282666733,"user_tz":-60,"elapsed":7,"user":{"displayName":"Daniele_Casciani","userId":"18232816629247177598"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["/gdrive/MyDrive/ANDL/Homework_2\n"]}],"source":["%cd /gdrive/MyDrive/ANDL/Homework_2"]},{"cell_type":"markdown","source":["# Libraries"],"metadata":{"id":"7bKbXlh-qMri"}},{"cell_type":"code","source":["!pip install keras_self_attention"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LXSjJjtm8vcS","executionInfo":{"status":"ok","timestamp":1671282674260,"user_tz":-60,"elapsed":7532,"user":{"displayName":"Daniele_Casciani","userId":"18232816629247177598"}},"outputId":"63632870-263b-4a7b-abd3-1610e25a247e"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting keras_self_attention\n","  Downloading keras-self-attention-0.51.0.tar.gz (11 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from keras_self_attention) (1.21.6)\n","Building wheels for collected packages: keras-self-attention\n","  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for keras-self-attention: filename=keras_self_attention-0.51.0-py3-none-any.whl size=18913 sha256=5a2a0e15f6ed0968282f4dac645868e1d7466dbb4feef9002573d000321082f9\n","  Stored in directory: /root/.cache/pip/wheels/ac/13/2d/3de7c76f618a8d162884ac5b726a8c2242ad88afa370f1e62f\n","Successfully built keras-self-attention\n","Installing collected packages: keras-self-attention\n","Successfully installed keras-self-attention-0.51.0\n"]}]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5kXC4rYkB8RA","outputId":"2b32f325-be1d-4574-8c30-6b2edb1f6d18","executionInfo":{"status":"ok","timestamp":1671282679503,"user_tz":-60,"elapsed":5249,"user":{"displayName":"Daniele_Casciani","userId":"18232816629247177598"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["2.9.2\n"]}],"source":["import tensorflow as tf\n","import numpy as np\n","import os\n","import random\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","plt.rc('font', size=16) \n","from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n","from sklearn.metrics import confusion_matrix\n","from sklearn.preprocessing import MinMaxScaler, StandardScaler\n","from sklearn.model_selection import train_test_split\n","from keras_self_attention import SeqSelfAttention, SeqWeightedAttention\n","from tensorflow.keras import regularizers\n","\n","import warnings\n","import logging\n","\n","tfk = tf.keras\n","tfkl = tf.keras.layers\n","print(tf.__version__)"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"qGwZea6qB_MB","executionInfo":{"status":"ok","timestamp":1671282679503,"user_tz":-60,"elapsed":8,"user":{"displayName":"Daniele_Casciani","userId":"18232816629247177598"}}},"outputs":[],"source":["# Random seed for reproducibility\n","seed = 42\n","\n","random.seed(seed)\n","os.environ['PYTHONHASHSEED'] = str(seed)\n","np.random.seed(seed)\n","tf.random.set_seed(seed)\n","tf.compat.v1.set_random_seed(seed)"]},{"cell_type":"markdown","source":["# Load Data"],"metadata":{"id":"Rq-BbC-HqCxq"}},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oc8CC3VJDMX6","outputId":"8dbea95d-fa49-43aa-8867-5e58b45f7595","executionInfo":{"status":"ok","timestamp":1671282680246,"user_tz":-60,"elapsed":750,"user":{"displayName":"Daniele_Casciani","userId":"18232816629247177598"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["((2429, 36, 6), (2429,))"]},"metadata":{},"execution_count":6}],"source":["X_train = np.load(\"x_train.npy\")\n","y_train = np.load(\"y_train.npy\")\n","\n","X_train.shape, y_train.shape"]},{"cell_type":"code","source":["df = pd.DataFrame()\n","\n","for t in range(0, 2429):\n","  temp_df = pd.DataFrame(X_train[t, :, :])\n","  temp_df[\"serie\"] = t\n","  temp_df[\"target\"] = y_train[t]\n","  df = pd.concat([df, temp_df])\n","df = df.rename(columns={0:\"x0\", 1:\"x1\", 2:\"x2\", 3:\"x3\", 4:\"x4\", 5:\"x5\"})\n","print(df)"],"metadata":{"id":"BgTqHCGoHuFi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671282688203,"user_tz":-60,"elapsed":7961,"user":{"displayName":"Daniele_Casciani","userId":"18232816629247177598"}},"outputId":"d82b904c-c2dc-4604-9d70-7038347f1c9a"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["          x0        x1        x2        x3        x4       x5  serie  target\n","0   17.59700   8.17130  -1.78420 -19.70600  -9.71350 -61.8870      0       0\n","1    0.22974  -5.89560  16.83700   5.03900   2.43320  58.9140      0       0\n","2  -29.65400 -21.29600  29.10300  -0.47503  75.39100  10.6650      0       0\n","3  -59.21000 -29.56500  17.97100 -45.60600  19.41200 -43.0140      0       0\n","4  -72.06400 -25.85500   0.46536 -59.15100 -29.10500 -11.9130      0       0\n","..       ...       ...       ...       ...       ...      ...    ...     ...\n","31   3.51190  -1.03240  -1.69900   4.15790   2.75430   5.9823   2428      11\n","32   0.70853  -0.94622  -1.91760   2.09480  -4.76750   7.4118   2428      11\n","33   0.20638   1.02870  -0.92512  -0.71633   1.73330   4.7638   2428      11\n","34   4.14290   1.09390  -0.18921   1.14100   0.93441   4.1742   2428      11\n","35  -1.55450  -1.21210   1.07050   1.32720   2.40100   8.4550   2428      11\n","\n","[87444 rows x 8 columns]\n"]}]},{"cell_type":"code","source":["df.head(1000)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"pn3jPSj6QOc-","executionInfo":{"status":"ok","timestamp":1671282688204,"user_tz":-60,"elapsed":19,"user":{"displayName":"Daniele_Casciani","userId":"18232816629247177598"}},"outputId":"e7a95507-3dcd-4fbd-d8e8-0eded77fc032"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["          x0       x1         x2        x3       x4      x5  serie  target\n","0   17.59700   8.1713  -1.784200 -19.70600  -9.7135 -61.887      0       0\n","1    0.22974  -5.8956  16.837000   5.03900   2.4332  58.914      0       0\n","2  -29.65400 -21.2960  29.103000  -0.47503  75.3910  10.665      0       0\n","3  -59.21000 -29.5650  17.971000 -45.60600  19.4120 -43.014      0       0\n","4  -72.06400 -25.8550   0.465360 -59.15100 -29.1050 -11.913      0       0\n","..       ...      ...        ...       ...      ...     ...    ...     ...\n","23  -4.56440   4.7574   1.666500  30.24900   3.5516  72.818     27       0\n","24   2.68540   9.7023   6.918400 -19.56100 -50.0060 -22.576     27       0\n","25  11.47700   8.9163  20.230000  -3.72380  66.5000  47.156     27       0\n","26  15.94500   3.9105  14.408000  25.17500   8.3458  87.464     27       0\n","27  13.23900  -2.1987  -0.038913   6.01700 -15.4140  25.051     27       0\n","\n","[1000 rows x 8 columns]"],"text/html":["\n","  <div id=\"df-479a0b81-7fcf-4025-9405-b683658dd7b5\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>x0</th>\n","      <th>x1</th>\n","      <th>x2</th>\n","      <th>x3</th>\n","      <th>x4</th>\n","      <th>x5</th>\n","      <th>serie</th>\n","      <th>target</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>17.59700</td>\n","      <td>8.1713</td>\n","      <td>-1.784200</td>\n","      <td>-19.70600</td>\n","      <td>-9.7135</td>\n","      <td>-61.887</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.22974</td>\n","      <td>-5.8956</td>\n","      <td>16.837000</td>\n","      <td>5.03900</td>\n","      <td>2.4332</td>\n","      <td>58.914</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>-29.65400</td>\n","      <td>-21.2960</td>\n","      <td>29.103000</td>\n","      <td>-0.47503</td>\n","      <td>75.3910</td>\n","      <td>10.665</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>-59.21000</td>\n","      <td>-29.5650</td>\n","      <td>17.971000</td>\n","      <td>-45.60600</td>\n","      <td>19.4120</td>\n","      <td>-43.014</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>-72.06400</td>\n","      <td>-25.8550</td>\n","      <td>0.465360</td>\n","      <td>-59.15100</td>\n","      <td>-29.1050</td>\n","      <td>-11.913</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>-4.56440</td>\n","      <td>4.7574</td>\n","      <td>1.666500</td>\n","      <td>30.24900</td>\n","      <td>3.5516</td>\n","      <td>72.818</td>\n","      <td>27</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>2.68540</td>\n","      <td>9.7023</td>\n","      <td>6.918400</td>\n","      <td>-19.56100</td>\n","      <td>-50.0060</td>\n","      <td>-22.576</td>\n","      <td>27</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>11.47700</td>\n","      <td>8.9163</td>\n","      <td>20.230000</td>\n","      <td>-3.72380</td>\n","      <td>66.5000</td>\n","      <td>47.156</td>\n","      <td>27</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>15.94500</td>\n","      <td>3.9105</td>\n","      <td>14.408000</td>\n","      <td>25.17500</td>\n","      <td>8.3458</td>\n","      <td>87.464</td>\n","      <td>27</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>13.23900</td>\n","      <td>-2.1987</td>\n","      <td>-0.038913</td>\n","      <td>6.01700</td>\n","      <td>-15.4140</td>\n","      <td>25.051</td>\n","      <td>27</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1000 rows × 8 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-479a0b81-7fcf-4025-9405-b683658dd7b5')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-479a0b81-7fcf-4025-9405-b683658dd7b5 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-479a0b81-7fcf-4025-9405-b683658dd7b5');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["# Add a date to use the seasonal_decompose module which requires a date for the points\n","df['date'] = pd.date_range(start='1/1/1900', periods=len(df))\n","# Convert daily column from just string to DateTime\n","df['date'] = pd.to_datetime(df['date'])\n","# Set the column 'Date' as index \n","df = df.set_index('date')\n","# Specify datetime frequency\n","df = df.asfreq('D')\n","print(df.shape)\n","df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":255},"id":"CFZ8oNX7FfI4","executionInfo":{"status":"ok","timestamp":1671282688204,"user_tz":-60,"elapsed":16,"user":{"displayName":"Daniele_Casciani","userId":"18232816629247177598"}},"outputId":"df1e3cce-52b4-489d-f69a-e1cc8f0da964"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["(87444, 8)\n"]},{"output_type":"execute_result","data":{"text/plain":["                  x0       x1        x2        x3       x4      x5  serie  \\\n","date                                                                        \n","1900-01-01  17.59700   8.1713  -1.78420 -19.70600  -9.7135 -61.887      0   \n","1900-01-02   0.22974  -5.8956  16.83700   5.03900   2.4332  58.914      0   \n","1900-01-03 -29.65400 -21.2960  29.10300  -0.47503  75.3910  10.665      0   \n","1900-01-04 -59.21000 -29.5650  17.97100 -45.60600  19.4120 -43.014      0   \n","1900-01-05 -72.06400 -25.8550   0.46536 -59.15100 -29.1050 -11.913      0   \n","\n","            target  \n","date                \n","1900-01-01       0  \n","1900-01-02       0  \n","1900-01-03       0  \n","1900-01-04       0  \n","1900-01-05       0  "],"text/html":["\n","  <div id=\"df-7bedd26a-9ec1-480a-906f-551602fa41bf\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>x0</th>\n","      <th>x1</th>\n","      <th>x2</th>\n","      <th>x3</th>\n","      <th>x4</th>\n","      <th>x5</th>\n","      <th>serie</th>\n","      <th>target</th>\n","    </tr>\n","    <tr>\n","      <th>date</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1900-01-01</th>\n","      <td>17.59700</td>\n","      <td>8.1713</td>\n","      <td>-1.78420</td>\n","      <td>-19.70600</td>\n","      <td>-9.7135</td>\n","      <td>-61.887</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1900-01-02</th>\n","      <td>0.22974</td>\n","      <td>-5.8956</td>\n","      <td>16.83700</td>\n","      <td>5.03900</td>\n","      <td>2.4332</td>\n","      <td>58.914</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1900-01-03</th>\n","      <td>-29.65400</td>\n","      <td>-21.2960</td>\n","      <td>29.10300</td>\n","      <td>-0.47503</td>\n","      <td>75.3910</td>\n","      <td>10.665</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1900-01-04</th>\n","      <td>-59.21000</td>\n","      <td>-29.5650</td>\n","      <td>17.97100</td>\n","      <td>-45.60600</td>\n","      <td>19.4120</td>\n","      <td>-43.014</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1900-01-05</th>\n","      <td>-72.06400</td>\n","      <td>-25.8550</td>\n","      <td>0.46536</td>\n","      <td>-59.15100</td>\n","      <td>-29.1050</td>\n","      <td>-11.913</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7bedd26a-9ec1-480a-906f-551602fa41bf')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-7bedd26a-9ec1-480a-906f-551602fa41bf button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-7bedd26a-9ec1-480a-906f-551602fa41bf');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["y_df_train = pd.DataFrame()\n","y_df_train['target'] = df['target'].values\n","y_df_train['serie'] = df['serie'].values\n","print(y_df_train)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eB1ahl9UFusk","executionInfo":{"status":"ok","timestamp":1671282688205,"user_tz":-60,"elapsed":14,"user":{"displayName":"Daniele_Casciani","userId":"18232816629247177598"}},"outputId":"bb464f21-b0ff-41f3-bd4c-33654d89932e"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["       target  serie\n","0           0      0\n","1           0      0\n","2           0      0\n","3           0      0\n","4           0      0\n","...       ...    ...\n","87439      11   2428\n","87440      11   2428\n","87441      11   2428\n","87442      11   2428\n","87443      11   2428\n","\n","[87444 rows x 2 columns]\n"]}]},{"cell_type":"code","source":["df = df.rolling(3).mean()"],"metadata":{"id":"0giO562EF-O5","executionInfo":{"status":"ok","timestamp":1671282688535,"user_tz":-60,"elapsed":342,"user":{"displayName":"Daniele_Casciani","userId":"18232816629247177598"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["df['target'] = y_df_train['target'].values\n","df['serie'] = y_df_train['serie'].values"],"metadata":{"id":"r40H0BctGHdD","executionInfo":{"status":"ok","timestamp":1671282688536,"user_tz":-60,"elapsed":7,"user":{"displayName":"Daniele_Casciani","userId":"18232816629247177598"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["df= df.fillna(0)"],"metadata":{"id":"PtvsCrQkGhld","executionInfo":{"status":"ok","timestamp":1671282688537,"user_tz":-60,"elapsed":7,"user":{"displayName":"Daniele_Casciani","userId":"18232816629247177598"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["df.head(100)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":455},"id":"UZ5Mu2hfGnnD","executionInfo":{"status":"ok","timestamp":1671282688835,"user_tz":-60,"elapsed":305,"user":{"displayName":"Daniele_Casciani","userId":"18232816629247177598"}},"outputId":"917fdee6-c886-4c7f-ea83-8f3a3615d569"},"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                   x0         x1         x2         x3         x4         x5  \\\n","date                                                                           \n","1900-01-01   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n","1900-01-02   0.000000   0.000000   0.000000   0.000000   0.000000   0.000000   \n","1900-01-03  -3.942420  -6.340100  14.718600  -5.047343  22.703567   2.564000   \n","1900-01-04 -29.544753 -18.918867  21.303667 -13.680677  32.412067   8.855000   \n","1900-01-05 -53.642667 -25.572000  15.846453 -35.077343  21.899333 -14.754000   \n","...               ...        ...        ...        ...        ...        ...   \n","1900-04-06  23.175667 -41.488333  -0.244790  -2.244867 -34.383000   7.767200   \n","1900-04-07  18.440667 -20.883633   1.193043   1.824517 -20.092800  12.596133   \n","1900-04-08   3.044000  -0.264967   5.621777  -1.006183  -9.958800   3.792333   \n","1900-04-09 -11.937667   7.645933  11.318167  -5.979017  -3.483133 -12.903667   \n","1900-04-10 -17.187333   3.260000  14.219667  -4.339900   3.501333 -10.864333   \n","\n","            serie  target  \n","date                       \n","1900-01-01      0       0  \n","1900-01-02      0       0  \n","1900-01-03      0       0  \n","1900-01-04      0       0  \n","1900-01-05      0       0  \n","...           ...     ...  \n","1900-04-06      2       0  \n","1900-04-07      2       0  \n","1900-04-08      2       0  \n","1900-04-09      2       0  \n","1900-04-10      2       0  \n","\n","[100 rows x 8 columns]"],"text/html":["\n","  <div id=\"df-8c0f2288-504c-4487-a248-70e91c0aea65\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>x0</th>\n","      <th>x1</th>\n","      <th>x2</th>\n","      <th>x3</th>\n","      <th>x4</th>\n","      <th>x5</th>\n","      <th>serie</th>\n","      <th>target</th>\n","    </tr>\n","    <tr>\n","      <th>date</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1900-01-01</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1900-01-02</th>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1900-01-03</th>\n","      <td>-3.942420</td>\n","      <td>-6.340100</td>\n","      <td>14.718600</td>\n","      <td>-5.047343</td>\n","      <td>22.703567</td>\n","      <td>2.564000</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1900-01-04</th>\n","      <td>-29.544753</td>\n","      <td>-18.918867</td>\n","      <td>21.303667</td>\n","      <td>-13.680677</td>\n","      <td>32.412067</td>\n","      <td>8.855000</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1900-01-05</th>\n","      <td>-53.642667</td>\n","      <td>-25.572000</td>\n","      <td>15.846453</td>\n","      <td>-35.077343</td>\n","      <td>21.899333</td>\n","      <td>-14.754000</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1900-04-06</th>\n","      <td>23.175667</td>\n","      <td>-41.488333</td>\n","      <td>-0.244790</td>\n","      <td>-2.244867</td>\n","      <td>-34.383000</td>\n","      <td>7.767200</td>\n","      <td>2</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1900-04-07</th>\n","      <td>18.440667</td>\n","      <td>-20.883633</td>\n","      <td>1.193043</td>\n","      <td>1.824517</td>\n","      <td>-20.092800</td>\n","      <td>12.596133</td>\n","      <td>2</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1900-04-08</th>\n","      <td>3.044000</td>\n","      <td>-0.264967</td>\n","      <td>5.621777</td>\n","      <td>-1.006183</td>\n","      <td>-9.958800</td>\n","      <td>3.792333</td>\n","      <td>2</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1900-04-09</th>\n","      <td>-11.937667</td>\n","      <td>7.645933</td>\n","      <td>11.318167</td>\n","      <td>-5.979017</td>\n","      <td>-3.483133</td>\n","      <td>-12.903667</td>\n","      <td>2</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1900-04-10</th>\n","      <td>-17.187333</td>\n","      <td>3.260000</td>\n","      <td>14.219667</td>\n","      <td>-4.339900</td>\n","      <td>3.501333</td>\n","      <td>-10.864333</td>\n","      <td>2</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>100 rows × 8 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8c0f2288-504c-4487-a248-70e91c0aea65')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-8c0f2288-504c-4487-a248-70e91c0aea65 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-8c0f2288-504c-4487-a248-70e91c0aea65');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":14}]},{"cell_type":"code","execution_count":15,"metadata":{"id":"GtH_WveYUqaL","executionInfo":{"status":"ok","timestamp":1671282688835,"user_tz":-60,"elapsed":3,"user":{"displayName":"Daniele_Casciani","userId":"18232816629247177598"}}},"outputs":[],"source":["window = 36\n","stride = 36"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"UMmTId30Uu2n","executionInfo":{"status":"ok","timestamp":1671282688835,"user_tz":-60,"elapsed":3,"user":{"displayName":"Daniele_Casciani","userId":"18232816629247177598"}}},"outputs":[],"source":["def build_sequences(df, window=36, stride=36):\n","    # Sanity check to avoid runtime errors\n","    assert window % stride == 0\n","    dataset = []\n","    labels = []\n","    for s in df['serie'].unique():\n","        # Take only meaningful features\n","        temp = df[df['serie'] == s][['x0', 'x1', 'x2', 'x3', 'x4', 'x5']].values\n","        # Save the label\n","        label = df[df['serie'] == s]['target'].values[0]\n","        # Compute padding length\n","        #padding_len = window - len(temp)%window\n","        # Create padding and concatenate it\n","        #padding = np.zeros((padding_len,6), dtype='float64')\n","        #temp = np.concatenate((temp,padding))\n","        # Build features windows with their corresponging labels\n","        idx = 0\n","        while idx+window <= len(temp):\n","            dataset.append(temp[idx:idx+window])\n","            labels.append(label)\n","            idx += stride\n","    dataset = np.array(dataset)\n","    labels = np.array(labels)\n","    return dataset, labels"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"sRvNlbGhPNUB","executionInfo":{"status":"ok","timestamp":1671282695764,"user_tz":-60,"elapsed":6931,"user":{"displayName":"Daniele_Casciani","userId":"18232816629247177598"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"f8234176-182b-4f1f-f67b-732934b09398"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["((2429, 36, 6), (2429,))"]},"metadata":{},"execution_count":17}],"source":["X_train, y_train = build_sequences(df, window, stride)\n","X_train.shape, y_train.shape"]},{"cell_type":"code","source":["X_train, X_test, y_train, y_test = train_test_split(X_train, y_train ,random_state=104, test_size=0.15, shuffle=True)\n","X_train.shape, y_train.shape, X_test.shape, y_test.shape"],"metadata":{"id":"mkaizmv__ssE","executionInfo":{"status":"ok","timestamp":1671282695764,"user_tz":-60,"elapsed":10,"user":{"displayName":"Daniele_Casciani","userId":"18232816629247177598"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"b0bb749b-49fe-41ff-be4c-eb84338c62c4"},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((2064, 36, 6), (2064,), (365, 36, 6), (365,))"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","execution_count":19,"metadata":{"id":"LJ0CeWc8PeNb","executionInfo":{"status":"ok","timestamp":1671282695765,"user_tz":-60,"elapsed":8,"user":{"displayName":"Daniele_Casciani","userId":"18232816629247177598"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"3f96bb99-513f-4317-e313-83f7a2dbf6fe"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["((2064, 36, 6), (2064, 12), (365, 36, 6), (365, 12))"]},"metadata":{},"execution_count":19}],"source":["# Convert the sparse labels to categorical values\n","y_train = tfk.utils.to_categorical(y_train)\n","y_test = tfk.utils.to_categorical(y_test)\n","X_train.shape, y_train.shape, X_test.shape, y_test.shape"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"2NRGcY4vPhQr","executionInfo":{"status":"ok","timestamp":1671282695765,"user_tz":-60,"elapsed":5,"user":{"displayName":"Daniele_Casciani","userId":"18232816629247177598"}}},"outputs":[],"source":["input_shape = X_train.shape[1:]\n","classes = y_train.shape[-1]\n","batch_size = 32\n","epochs = 200"]},{"cell_type":"code","source":["def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n","    # Attention and Normalization\n","    x = tfkl.MultiHeadAttention(\n","        key_dim=head_size, num_heads=num_heads, dropout=dropout\n","    )(inputs, inputs)\n","    x = tfkl.Dropout(dropout)(x)\n","    x = tfkl.LayerNormalization(epsilon=1e-6)(x)\n","    res = x + inputs\n","\n","    # Feed Forward Part\n","    x = tfkl.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(res)\n","    x = tfkl.Dropout(dropout)(x)\n","    x = tfkl.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n","    x = tfkl.LayerNormalization(epsilon=1e-6)(x)\n","    return x + res"],"metadata":{"id":"NvM2TdCNNjYV","executionInfo":{"status":"ok","timestamp":1671282918696,"user_tz":-60,"elapsed":190,"user":{"displayName":"Daniele_Casciani","userId":"18232816629247177598"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["def build_model(\n","    input_shape,\n","    head_size,\n","    num_heads,\n","    ff_dim,\n","    num_transformer_blocks,\n","    mlp_units,\n","    dropout=0,\n","    mlp_dropout=0,\n","):\n","    inputs = tfk.Input(shape=input_shape)\n","    x = inputs\n","    for _ in range(num_transformer_blocks):\n","        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n","\n","    x = tfkl.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n","    for dim in mlp_units:\n","        x = tfkl.Dense(dim, activation=\"relu\")(x)\n","        x = tfkl.Dropout(mlp_dropout)(x)\n","    outputs = tfkl.Dense(classes, activation=\"softmax\")(x)\n","    return tfk.Model(inputs, outputs)"],"metadata":{"id":"ZE1zKi9bNm0Z","executionInfo":{"status":"ok","timestamp":1671282920135,"user_tz":-60,"elapsed":206,"user":{"displayName":"Daniele_Casciani","userId":"18232816629247177598"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["model = build_model(\n","    input_shape,\n","    head_size=256,\n","    num_heads=4,\n","    ff_dim=4,\n","    num_transformer_blocks=16,\n","    mlp_units=[128],\n","    mlp_dropout=0.4,\n","    dropout=0.25,\n",")\n","\n","# Compile the model\n","model.compile(loss=tfk.losses.CategoricalCrossentropy(), optimizer=tfk.optimizers.Adam(), metrics='accuracy')\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jogm76AqN-E8","executionInfo":{"status":"ok","timestamp":1671287775260,"user_tz":-60,"elapsed":3647,"user":{"displayName":"Daniele_Casciani","userId":"18232816629247177598"}},"outputId":"88066f84-d532-4566-a272-4860df50a704"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_4\"\n","__________________________________________________________________________________________________\n"," Layer (type)                   Output Shape         Param #     Connected to                     \n","==================================================================================================\n"," input_5 (InputLayer)           [(None, 36, 6)]      0           []                               \n","                                                                                                  \n"," multi_head_attention_20 (Multi  (None, 36, 6)       27654       ['input_5[0][0]',                \n"," HeadAttention)                                                   'input_5[0][0]']                \n","                                                                                                  \n"," dropout_44 (Dropout)           (None, 36, 6)        0           ['multi_head_attention_20[0][0]']\n","                                                                                                  \n"," layer_normalization_40 (LayerN  (None, 36, 6)       12          ['dropout_44[0][0]']             \n"," ormalization)                                                                                    \n","                                                                                                  \n"," tf.__operators__.add_40 (TFOpL  (None, 36, 6)       0           ['layer_normalization_40[0][0]', \n"," ambda)                                                           'input_5[0][0]']                \n","                                                                                                  \n"," conv1d_40 (Conv1D)             (None, 36, 4)        28          ['tf.__operators__.add_40[0][0]']\n","                                                                                                  \n"," dropout_45 (Dropout)           (None, 36, 4)        0           ['conv1d_40[0][0]']              \n","                                                                                                  \n"," conv1d_41 (Conv1D)             (None, 36, 6)        30          ['dropout_45[0][0]']             \n","                                                                                                  \n"," layer_normalization_41 (LayerN  (None, 36, 6)       12          ['conv1d_41[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," tf.__operators__.add_41 (TFOpL  (None, 36, 6)       0           ['layer_normalization_41[0][0]', \n"," ambda)                                                           'tf.__operators__.add_40[0][0]']\n","                                                                                                  \n"," multi_head_attention_21 (Multi  (None, 36, 6)       27654       ['tf.__operators__.add_41[0][0]',\n"," HeadAttention)                                                   'tf.__operators__.add_41[0][0]']\n","                                                                                                  \n"," dropout_46 (Dropout)           (None, 36, 6)        0           ['multi_head_attention_21[0][0]']\n","                                                                                                  \n"," layer_normalization_42 (LayerN  (None, 36, 6)       12          ['dropout_46[0][0]']             \n"," ormalization)                                                                                    \n","                                                                                                  \n"," tf.__operators__.add_42 (TFOpL  (None, 36, 6)       0           ['layer_normalization_42[0][0]', \n"," ambda)                                                           'tf.__operators__.add_41[0][0]']\n","                                                                                                  \n"," conv1d_42 (Conv1D)             (None, 36, 4)        28          ['tf.__operators__.add_42[0][0]']\n","                                                                                                  \n"," dropout_47 (Dropout)           (None, 36, 4)        0           ['conv1d_42[0][0]']              \n","                                                                                                  \n"," conv1d_43 (Conv1D)             (None, 36, 6)        30          ['dropout_47[0][0]']             \n","                                                                                                  \n"," layer_normalization_43 (LayerN  (None, 36, 6)       12          ['conv1d_43[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," tf.__operators__.add_43 (TFOpL  (None, 36, 6)       0           ['layer_normalization_43[0][0]', \n"," ambda)                                                           'tf.__operators__.add_42[0][0]']\n","                                                                                                  \n"," multi_head_attention_22 (Multi  (None, 36, 6)       27654       ['tf.__operators__.add_43[0][0]',\n"," HeadAttention)                                                   'tf.__operators__.add_43[0][0]']\n","                                                                                                  \n"," dropout_48 (Dropout)           (None, 36, 6)        0           ['multi_head_attention_22[0][0]']\n","                                                                                                  \n"," layer_normalization_44 (LayerN  (None, 36, 6)       12          ['dropout_48[0][0]']             \n"," ormalization)                                                                                    \n","                                                                                                  \n"," tf.__operators__.add_44 (TFOpL  (None, 36, 6)       0           ['layer_normalization_44[0][0]', \n"," ambda)                                                           'tf.__operators__.add_43[0][0]']\n","                                                                                                  \n"," conv1d_44 (Conv1D)             (None, 36, 4)        28          ['tf.__operators__.add_44[0][0]']\n","                                                                                                  \n"," dropout_49 (Dropout)           (None, 36, 4)        0           ['conv1d_44[0][0]']              \n","                                                                                                  \n"," conv1d_45 (Conv1D)             (None, 36, 6)        30          ['dropout_49[0][0]']             \n","                                                                                                  \n"," layer_normalization_45 (LayerN  (None, 36, 6)       12          ['conv1d_45[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," tf.__operators__.add_45 (TFOpL  (None, 36, 6)       0           ['layer_normalization_45[0][0]', \n"," ambda)                                                           'tf.__operators__.add_44[0][0]']\n","                                                                                                  \n"," multi_head_attention_23 (Multi  (None, 36, 6)       27654       ['tf.__operators__.add_45[0][0]',\n"," HeadAttention)                                                   'tf.__operators__.add_45[0][0]']\n","                                                                                                  \n"," dropout_50 (Dropout)           (None, 36, 6)        0           ['multi_head_attention_23[0][0]']\n","                                                                                                  \n"," layer_normalization_46 (LayerN  (None, 36, 6)       12          ['dropout_50[0][0]']             \n"," ormalization)                                                                                    \n","                                                                                                  \n"," tf.__operators__.add_46 (TFOpL  (None, 36, 6)       0           ['layer_normalization_46[0][0]', \n"," ambda)                                                           'tf.__operators__.add_45[0][0]']\n","                                                                                                  \n"," conv1d_46 (Conv1D)             (None, 36, 4)        28          ['tf.__operators__.add_46[0][0]']\n","                                                                                                  \n"," dropout_51 (Dropout)           (None, 36, 4)        0           ['conv1d_46[0][0]']              \n","                                                                                                  \n"," conv1d_47 (Conv1D)             (None, 36, 6)        30          ['dropout_51[0][0]']             \n","                                                                                                  \n"," layer_normalization_47 (LayerN  (None, 36, 6)       12          ['conv1d_47[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," tf.__operators__.add_47 (TFOpL  (None, 36, 6)       0           ['layer_normalization_47[0][0]', \n"," ambda)                                                           'tf.__operators__.add_46[0][0]']\n","                                                                                                  \n"," multi_head_attention_24 (Multi  (None, 36, 6)       27654       ['tf.__operators__.add_47[0][0]',\n"," HeadAttention)                                                   'tf.__operators__.add_47[0][0]']\n","                                                                                                  \n"," dropout_52 (Dropout)           (None, 36, 6)        0           ['multi_head_attention_24[0][0]']\n","                                                                                                  \n"," layer_normalization_48 (LayerN  (None, 36, 6)       12          ['dropout_52[0][0]']             \n"," ormalization)                                                                                    \n","                                                                                                  \n"," tf.__operators__.add_48 (TFOpL  (None, 36, 6)       0           ['layer_normalization_48[0][0]', \n"," ambda)                                                           'tf.__operators__.add_47[0][0]']\n","                                                                                                  \n"," conv1d_48 (Conv1D)             (None, 36, 4)        28          ['tf.__operators__.add_48[0][0]']\n","                                                                                                  \n"," dropout_53 (Dropout)           (None, 36, 4)        0           ['conv1d_48[0][0]']              \n","                                                                                                  \n"," conv1d_49 (Conv1D)             (None, 36, 6)        30          ['dropout_53[0][0]']             \n","                                                                                                  \n"," layer_normalization_49 (LayerN  (None, 36, 6)       12          ['conv1d_49[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," tf.__operators__.add_49 (TFOpL  (None, 36, 6)       0           ['layer_normalization_49[0][0]', \n"," ambda)                                                           'tf.__operators__.add_48[0][0]']\n","                                                                                                  \n"," multi_head_attention_25 (Multi  (None, 36, 6)       27654       ['tf.__operators__.add_49[0][0]',\n"," HeadAttention)                                                   'tf.__operators__.add_49[0][0]']\n","                                                                                                  \n"," dropout_54 (Dropout)           (None, 36, 6)        0           ['multi_head_attention_25[0][0]']\n","                                                                                                  \n"," layer_normalization_50 (LayerN  (None, 36, 6)       12          ['dropout_54[0][0]']             \n"," ormalization)                                                                                    \n","                                                                                                  \n"," tf.__operators__.add_50 (TFOpL  (None, 36, 6)       0           ['layer_normalization_50[0][0]', \n"," ambda)                                                           'tf.__operators__.add_49[0][0]']\n","                                                                                                  \n"," conv1d_50 (Conv1D)             (None, 36, 4)        28          ['tf.__operators__.add_50[0][0]']\n","                                                                                                  \n"," dropout_55 (Dropout)           (None, 36, 4)        0           ['conv1d_50[0][0]']              \n","                                                                                                  \n"," conv1d_51 (Conv1D)             (None, 36, 6)        30          ['dropout_55[0][0]']             \n","                                                                                                  \n"," layer_normalization_51 (LayerN  (None, 36, 6)       12          ['conv1d_51[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," tf.__operators__.add_51 (TFOpL  (None, 36, 6)       0           ['layer_normalization_51[0][0]', \n"," ambda)                                                           'tf.__operators__.add_50[0][0]']\n","                                                                                                  \n"," multi_head_attention_26 (Multi  (None, 36, 6)       27654       ['tf.__operators__.add_51[0][0]',\n"," HeadAttention)                                                   'tf.__operators__.add_51[0][0]']\n","                                                                                                  \n"," dropout_56 (Dropout)           (None, 36, 6)        0           ['multi_head_attention_26[0][0]']\n","                                                                                                  \n"," layer_normalization_52 (LayerN  (None, 36, 6)       12          ['dropout_56[0][0]']             \n"," ormalization)                                                                                    \n","                                                                                                  \n"," tf.__operators__.add_52 (TFOpL  (None, 36, 6)       0           ['layer_normalization_52[0][0]', \n"," ambda)                                                           'tf.__operators__.add_51[0][0]']\n","                                                                                                  \n"," conv1d_52 (Conv1D)             (None, 36, 4)        28          ['tf.__operators__.add_52[0][0]']\n","                                                                                                  \n"," dropout_57 (Dropout)           (None, 36, 4)        0           ['conv1d_52[0][0]']              \n","                                                                                                  \n"," conv1d_53 (Conv1D)             (None, 36, 6)        30          ['dropout_57[0][0]']             \n","                                                                                                  \n"," layer_normalization_53 (LayerN  (None, 36, 6)       12          ['conv1d_53[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," tf.__operators__.add_53 (TFOpL  (None, 36, 6)       0           ['layer_normalization_53[0][0]', \n"," ambda)                                                           'tf.__operators__.add_52[0][0]']\n","                                                                                                  \n"," multi_head_attention_27 (Multi  (None, 36, 6)       27654       ['tf.__operators__.add_53[0][0]',\n"," HeadAttention)                                                   'tf.__operators__.add_53[0][0]']\n","                                                                                                  \n"," dropout_58 (Dropout)           (None, 36, 6)        0           ['multi_head_attention_27[0][0]']\n","                                                                                                  \n"," layer_normalization_54 (LayerN  (None, 36, 6)       12          ['dropout_58[0][0]']             \n"," ormalization)                                                                                    \n","                                                                                                  \n"," tf.__operators__.add_54 (TFOpL  (None, 36, 6)       0           ['layer_normalization_54[0][0]', \n"," ambda)                                                           'tf.__operators__.add_53[0][0]']\n","                                                                                                  \n"," conv1d_54 (Conv1D)             (None, 36, 4)        28          ['tf.__operators__.add_54[0][0]']\n","                                                                                                  \n"," dropout_59 (Dropout)           (None, 36, 4)        0           ['conv1d_54[0][0]']              \n","                                                                                                  \n"," conv1d_55 (Conv1D)             (None, 36, 6)        30          ['dropout_59[0][0]']             \n","                                                                                                  \n"," layer_normalization_55 (LayerN  (None, 36, 6)       12          ['conv1d_55[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," tf.__operators__.add_55 (TFOpL  (None, 36, 6)       0           ['layer_normalization_55[0][0]', \n"," ambda)                                                           'tf.__operators__.add_54[0][0]']\n","                                                                                                  \n"," multi_head_attention_28 (Multi  (None, 36, 6)       27654       ['tf.__operators__.add_55[0][0]',\n"," HeadAttention)                                                   'tf.__operators__.add_55[0][0]']\n","                                                                                                  \n"," dropout_60 (Dropout)           (None, 36, 6)        0           ['multi_head_attention_28[0][0]']\n","                                                                                                  \n"," layer_normalization_56 (LayerN  (None, 36, 6)       12          ['dropout_60[0][0]']             \n"," ormalization)                                                                                    \n","                                                                                                  \n"," tf.__operators__.add_56 (TFOpL  (None, 36, 6)       0           ['layer_normalization_56[0][0]', \n"," ambda)                                                           'tf.__operators__.add_55[0][0]']\n","                                                                                                  \n"," conv1d_56 (Conv1D)             (None, 36, 4)        28          ['tf.__operators__.add_56[0][0]']\n","                                                                                                  \n"," dropout_61 (Dropout)           (None, 36, 4)        0           ['conv1d_56[0][0]']              \n","                                                                                                  \n"," conv1d_57 (Conv1D)             (None, 36, 6)        30          ['dropout_61[0][0]']             \n","                                                                                                  \n"," layer_normalization_57 (LayerN  (None, 36, 6)       12          ['conv1d_57[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," tf.__operators__.add_57 (TFOpL  (None, 36, 6)       0           ['layer_normalization_57[0][0]', \n"," ambda)                                                           'tf.__operators__.add_56[0][0]']\n","                                                                                                  \n"," multi_head_attention_29 (Multi  (None, 36, 6)       27654       ['tf.__operators__.add_57[0][0]',\n"," HeadAttention)                                                   'tf.__operators__.add_57[0][0]']\n","                                                                                                  \n"," dropout_62 (Dropout)           (None, 36, 6)        0           ['multi_head_attention_29[0][0]']\n","                                                                                                  \n"," layer_normalization_58 (LayerN  (None, 36, 6)       12          ['dropout_62[0][0]']             \n"," ormalization)                                                                                    \n","                                                                                                  \n"," tf.__operators__.add_58 (TFOpL  (None, 36, 6)       0           ['layer_normalization_58[0][0]', \n"," ambda)                                                           'tf.__operators__.add_57[0][0]']\n","                                                                                                  \n"," conv1d_58 (Conv1D)             (None, 36, 4)        28          ['tf.__operators__.add_58[0][0]']\n","                                                                                                  \n"," dropout_63 (Dropout)           (None, 36, 4)        0           ['conv1d_58[0][0]']              \n","                                                                                                  \n"," conv1d_59 (Conv1D)             (None, 36, 6)        30          ['dropout_63[0][0]']             \n","                                                                                                  \n"," layer_normalization_59 (LayerN  (None, 36, 6)       12          ['conv1d_59[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," tf.__operators__.add_59 (TFOpL  (None, 36, 6)       0           ['layer_normalization_59[0][0]', \n"," ambda)                                                           'tf.__operators__.add_58[0][0]']\n","                                                                                                  \n"," multi_head_attention_30 (Multi  (None, 36, 6)       27654       ['tf.__operators__.add_59[0][0]',\n"," HeadAttention)                                                   'tf.__operators__.add_59[0][0]']\n","                                                                                                  \n"," dropout_64 (Dropout)           (None, 36, 6)        0           ['multi_head_attention_30[0][0]']\n","                                                                                                  \n"," layer_normalization_60 (LayerN  (None, 36, 6)       12          ['dropout_64[0][0]']             \n"," ormalization)                                                                                    \n","                                                                                                  \n"," tf.__operators__.add_60 (TFOpL  (None, 36, 6)       0           ['layer_normalization_60[0][0]', \n"," ambda)                                                           'tf.__operators__.add_59[0][0]']\n","                                                                                                  \n"," conv1d_60 (Conv1D)             (None, 36, 4)        28          ['tf.__operators__.add_60[0][0]']\n","                                                                                                  \n"," dropout_65 (Dropout)           (None, 36, 4)        0           ['conv1d_60[0][0]']              \n","                                                                                                  \n"," conv1d_61 (Conv1D)             (None, 36, 6)        30          ['dropout_65[0][0]']             \n","                                                                                                  \n"," layer_normalization_61 (LayerN  (None, 36, 6)       12          ['conv1d_61[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," tf.__operators__.add_61 (TFOpL  (None, 36, 6)       0           ['layer_normalization_61[0][0]', \n"," ambda)                                                           'tf.__operators__.add_60[0][0]']\n","                                                                                                  \n"," multi_head_attention_31 (Multi  (None, 36, 6)       27654       ['tf.__operators__.add_61[0][0]',\n"," HeadAttention)                                                   'tf.__operators__.add_61[0][0]']\n","                                                                                                  \n"," dropout_66 (Dropout)           (None, 36, 6)        0           ['multi_head_attention_31[0][0]']\n","                                                                                                  \n"," layer_normalization_62 (LayerN  (None, 36, 6)       12          ['dropout_66[0][0]']             \n"," ormalization)                                                                                    \n","                                                                                                  \n"," tf.__operators__.add_62 (TFOpL  (None, 36, 6)       0           ['layer_normalization_62[0][0]', \n"," ambda)                                                           'tf.__operators__.add_61[0][0]']\n","                                                                                                  \n"," conv1d_62 (Conv1D)             (None, 36, 4)        28          ['tf.__operators__.add_62[0][0]']\n","                                                                                                  \n"," dropout_67 (Dropout)           (None, 36, 4)        0           ['conv1d_62[0][0]']              \n","                                                                                                  \n"," conv1d_63 (Conv1D)             (None, 36, 6)        30          ['dropout_67[0][0]']             \n","                                                                                                  \n"," layer_normalization_63 (LayerN  (None, 36, 6)       12          ['conv1d_63[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," tf.__operators__.add_63 (TFOpL  (None, 36, 6)       0           ['layer_normalization_63[0][0]', \n"," ambda)                                                           'tf.__operators__.add_62[0][0]']\n","                                                                                                  \n"," multi_head_attention_32 (Multi  (None, 36, 6)       27654       ['tf.__operators__.add_63[0][0]',\n"," HeadAttention)                                                   'tf.__operators__.add_63[0][0]']\n","                                                                                                  \n"," dropout_68 (Dropout)           (None, 36, 6)        0           ['multi_head_attention_32[0][0]']\n","                                                                                                  \n"," layer_normalization_64 (LayerN  (None, 36, 6)       12          ['dropout_68[0][0]']             \n"," ormalization)                                                                                    \n","                                                                                                  \n"," tf.__operators__.add_64 (TFOpL  (None, 36, 6)       0           ['layer_normalization_64[0][0]', \n"," ambda)                                                           'tf.__operators__.add_63[0][0]']\n","                                                                                                  \n"," conv1d_64 (Conv1D)             (None, 36, 4)        28          ['tf.__operators__.add_64[0][0]']\n","                                                                                                  \n"," dropout_69 (Dropout)           (None, 36, 4)        0           ['conv1d_64[0][0]']              \n","                                                                                                  \n"," conv1d_65 (Conv1D)             (None, 36, 6)        30          ['dropout_69[0][0]']             \n","                                                                                                  \n"," layer_normalization_65 (LayerN  (None, 36, 6)       12          ['conv1d_65[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," tf.__operators__.add_65 (TFOpL  (None, 36, 6)       0           ['layer_normalization_65[0][0]', \n"," ambda)                                                           'tf.__operators__.add_64[0][0]']\n","                                                                                                  \n"," multi_head_attention_33 (Multi  (None, 36, 6)       27654       ['tf.__operators__.add_65[0][0]',\n"," HeadAttention)                                                   'tf.__operators__.add_65[0][0]']\n","                                                                                                  \n"," dropout_70 (Dropout)           (None, 36, 6)        0           ['multi_head_attention_33[0][0]']\n","                                                                                                  \n"," layer_normalization_66 (LayerN  (None, 36, 6)       12          ['dropout_70[0][0]']             \n"," ormalization)                                                                                    \n","                                                                                                  \n"," tf.__operators__.add_66 (TFOpL  (None, 36, 6)       0           ['layer_normalization_66[0][0]', \n"," ambda)                                                           'tf.__operators__.add_65[0][0]']\n","                                                                                                  \n"," conv1d_66 (Conv1D)             (None, 36, 4)        28          ['tf.__operators__.add_66[0][0]']\n","                                                                                                  \n"," dropout_71 (Dropout)           (None, 36, 4)        0           ['conv1d_66[0][0]']              \n","                                                                                                  \n"," conv1d_67 (Conv1D)             (None, 36, 6)        30          ['dropout_71[0][0]']             \n","                                                                                                  \n"," layer_normalization_67 (LayerN  (None, 36, 6)       12          ['conv1d_67[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," tf.__operators__.add_67 (TFOpL  (None, 36, 6)       0           ['layer_normalization_67[0][0]', \n"," ambda)                                                           'tf.__operators__.add_66[0][0]']\n","                                                                                                  \n"," multi_head_attention_34 (Multi  (None, 36, 6)       27654       ['tf.__operators__.add_67[0][0]',\n"," HeadAttention)                                                   'tf.__operators__.add_67[0][0]']\n","                                                                                                  \n"," dropout_72 (Dropout)           (None, 36, 6)        0           ['multi_head_attention_34[0][0]']\n","                                                                                                  \n"," layer_normalization_68 (LayerN  (None, 36, 6)       12          ['dropout_72[0][0]']             \n"," ormalization)                                                                                    \n","                                                                                                  \n"," tf.__operators__.add_68 (TFOpL  (None, 36, 6)       0           ['layer_normalization_68[0][0]', \n"," ambda)                                                           'tf.__operators__.add_67[0][0]']\n","                                                                                                  \n"," conv1d_68 (Conv1D)             (None, 36, 4)        28          ['tf.__operators__.add_68[0][0]']\n","                                                                                                  \n"," dropout_73 (Dropout)           (None, 36, 4)        0           ['conv1d_68[0][0]']              \n","                                                                                                  \n"," conv1d_69 (Conv1D)             (None, 36, 6)        30          ['dropout_73[0][0]']             \n","                                                                                                  \n"," layer_normalization_69 (LayerN  (None, 36, 6)       12          ['conv1d_69[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," tf.__operators__.add_69 (TFOpL  (None, 36, 6)       0           ['layer_normalization_69[0][0]', \n"," ambda)                                                           'tf.__operators__.add_68[0][0]']\n","                                                                                                  \n"," multi_head_attention_35 (Multi  (None, 36, 6)       27654       ['tf.__operators__.add_69[0][0]',\n"," HeadAttention)                                                   'tf.__operators__.add_69[0][0]']\n","                                                                                                  \n"," dropout_74 (Dropout)           (None, 36, 6)        0           ['multi_head_attention_35[0][0]']\n","                                                                                                  \n"," layer_normalization_70 (LayerN  (None, 36, 6)       12          ['dropout_74[0][0]']             \n"," ormalization)                                                                                    \n","                                                                                                  \n"," tf.__operators__.add_70 (TFOpL  (None, 36, 6)       0           ['layer_normalization_70[0][0]', \n"," ambda)                                                           'tf.__operators__.add_69[0][0]']\n","                                                                                                  \n"," conv1d_70 (Conv1D)             (None, 36, 4)        28          ['tf.__operators__.add_70[0][0]']\n","                                                                                                  \n"," dropout_75 (Dropout)           (None, 36, 4)        0           ['conv1d_70[0][0]']              \n","                                                                                                  \n"," conv1d_71 (Conv1D)             (None, 36, 6)        30          ['dropout_75[0][0]']             \n","                                                                                                  \n"," layer_normalization_71 (LayerN  (None, 36, 6)       12          ['conv1d_71[0][0]']              \n"," ormalization)                                                                                    \n","                                                                                                  \n"," tf.__operators__.add_71 (TFOpL  (None, 36, 6)       0           ['layer_normalization_71[0][0]', \n"," ambda)                                                           'tf.__operators__.add_70[0][0]']\n","                                                                                                  \n"," global_average_pooling1d_4 (Gl  (None, 36)          0           ['tf.__operators__.add_71[0][0]']\n"," obalAveragePooling1D)                                                                            \n","                                                                                                  \n"," dense_8 (Dense)                (None, 128)          4736        ['global_average_pooling1d_4[0][0\n","                                                                 ]']                              \n","                                                                                                  \n"," dropout_76 (Dropout)           (None, 128)          0           ['dense_8[0][0]']                \n","                                                                                                  \n"," dense_9 (Dense)                (None, 12)           1548        ['dropout_76[0][0]']             \n","                                                                                                  \n","==================================================================================================\n","Total params: 450,060\n","Trainable params: 450,060\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tg-Od7m5R8Zc","outputId":"63390473-23e9-4c86-91c6-b5794e6771a5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/200\n","59/59 [==============================] - 106s 2s/step - loss: 82.3506 - accuracy: 0.1169 - val_loss: 70.5882 - val_accuracy: 0.1449 - lr: 0.0010\n","Epoch 2/200\n","59/59 [==============================] - 81s 1s/step - loss: 57.5976 - accuracy: 0.1330 - val_loss: 60.4164 - val_accuracy: 0.1643 - lr: 0.0010\n","Epoch 3/200\n","59/59 [==============================] - 80s 1s/step - loss: 52.5278 - accuracy: 0.1680 - val_loss: 40.5468 - val_accuracy: 0.1449 - lr: 0.0010\n","Epoch 4/200\n","59/59 [==============================] - 80s 1s/step - loss: 46.7248 - accuracy: 0.1524 - val_loss: 43.2078 - val_accuracy: 0.1739 - lr: 0.0010\n","Epoch 5/200\n","59/59 [==============================] - 84s 1s/step - loss: 40.7847 - accuracy: 0.1858 - val_loss: 43.5698 - val_accuracy: 0.2319 - lr: 0.0010\n","Epoch 6/200\n","59/59 [==============================] - 81s 1s/step - loss: 34.3684 - accuracy: 0.1949 - val_loss: 42.0971 - val_accuracy: 0.2029 - lr: 0.0010\n","Epoch 7/200\n","59/59 [==============================] - 81s 1s/step - loss: 27.6047 - accuracy: 0.1928 - val_loss: 28.2000 - val_accuracy: 0.2464 - lr: 0.0010\n","Epoch 8/200\n","59/59 [==============================] - 84s 1s/step - loss: 25.1962 - accuracy: 0.1960 - val_loss: 26.7978 - val_accuracy: 0.2560 - lr: 0.0010\n","Epoch 9/200\n","59/59 [==============================] - 83s 1s/step - loss: 22.9481 - accuracy: 0.2149 - val_loss: 23.5043 - val_accuracy: 0.2657 - lr: 0.0010\n","Epoch 10/200\n","59/59 [==============================] - 81s 1s/step - loss: 21.3626 - accuracy: 0.2429 - val_loss: 24.1373 - val_accuracy: 0.2464 - lr: 0.0010\n","Epoch 11/200\n","59/59 [==============================] - 82s 1s/step - loss: 20.6690 - accuracy: 0.2332 - val_loss: 26.2463 - val_accuracy: 0.2705 - lr: 0.0010\n","Epoch 12/200\n","59/59 [==============================] - 81s 1s/step - loss: 18.6258 - accuracy: 0.2186 - val_loss: 23.8779 - val_accuracy: 0.3092 - lr: 0.0010\n","Epoch 13/200\n","59/59 [==============================] - 81s 1s/step - loss: 17.8500 - accuracy: 0.2267 - val_loss: 25.3919 - val_accuracy: 0.2995 - lr: 0.0010\n","Epoch 14/200\n","59/59 [==============================] - 83s 1s/step - loss: 17.3111 - accuracy: 0.2671 - val_loss: 25.3225 - val_accuracy: 0.2850 - lr: 0.0010\n","Epoch 15/200\n","59/59 [==============================] - 80s 1s/step - loss: 16.9557 - accuracy: 0.2612 - val_loss: 26.7583 - val_accuracy: 0.2899 - lr: 0.0010\n","Epoch 16/200\n","59/59 [==============================] - 80s 1s/step - loss: 15.1871 - accuracy: 0.2499 - val_loss: 28.5167 - val_accuracy: 0.3188 - lr: 0.0010\n","Epoch 17/200\n","59/59 [==============================] - 81s 1s/step - loss: 14.1839 - accuracy: 0.2741 - val_loss: 26.9789 - val_accuracy: 0.2754 - lr: 0.0010\n","Epoch 18/200\n","59/59 [==============================] - 81s 1s/step - loss: 13.5361 - accuracy: 0.2897 - val_loss: 24.1744 - val_accuracy: 0.3043 - lr: 0.0010\n","Epoch 19/200\n","59/59 [==============================] - 81s 1s/step - loss: 12.5017 - accuracy: 0.2768 - val_loss: 26.3045 - val_accuracy: 0.2947 - lr: 0.0010\n","Epoch 20/200\n","59/59 [==============================] - 84s 1s/step - loss: 11.8218 - accuracy: 0.2843 - val_loss: 24.7271 - val_accuracy: 0.2899 - lr: 0.0010\n","Epoch 21/200\n","59/59 [==============================] - 81s 1s/step - loss: 10.9435 - accuracy: 0.2967 - val_loss: 23.1814 - val_accuracy: 0.3188 - lr: 0.0010\n","Epoch 22/200\n","59/59 [==============================] - 81s 1s/step - loss: 10.7125 - accuracy: 0.2736 - val_loss: 22.6774 - val_accuracy: 0.3478 - lr: 5.0000e-04\n","Epoch 23/200\n","59/59 [==============================] - 82s 1s/step - loss: 11.7029 - accuracy: 0.2994 - val_loss: 23.3417 - val_accuracy: 0.3671 - lr: 5.0000e-04\n","Epoch 24/200\n","59/59 [==============================] - 81s 1s/step - loss: 11.1819 - accuracy: 0.3059 - val_loss: 21.3267 - val_accuracy: 0.3430 - lr: 5.0000e-04\n","Epoch 25/200\n","59/59 [==============================] - 81s 1s/step - loss: 9.4838 - accuracy: 0.3107 - val_loss: 22.8747 - val_accuracy: 0.3043 - lr: 5.0000e-04\n","Epoch 26/200\n","59/59 [==============================] - 83s 1s/step - loss: 10.6105 - accuracy: 0.3048 - val_loss: 20.6895 - val_accuracy: 0.3237 - lr: 5.0000e-04\n","Epoch 27/200\n","59/59 [==============================] - 81s 1s/step - loss: 8.5857 - accuracy: 0.3091 - val_loss: 20.0456 - val_accuracy: 0.2947 - lr: 5.0000e-04\n","Epoch 28/200\n","59/59 [==============================] - 81s 1s/step - loss: 8.7251 - accuracy: 0.3215 - val_loss: 21.8167 - val_accuracy: 0.2947 - lr: 5.0000e-04\n","Epoch 29/200\n","59/59 [==============================] - 80s 1s/step - loss: 8.2666 - accuracy: 0.3436 - val_loss: 21.9096 - val_accuracy: 0.3092 - lr: 2.5000e-04\n","Epoch 30/200\n","59/59 [==============================] - ETA: 0s - loss: 8.1562 - accuracy: 0.3317"]}],"source":["# Train the model\n","history = model.fit(\n","    x = X_train,\n","    y = y_train,\n","    batch_size = batch_size,\n","    epochs = epochs,\n","    validation_split=.1,\n","    #class_weight=class_weight,\n","    callbacks = [\n","        tfk.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=20, restore_best_weights=True),\n","        tfk.callbacks.ReduceLROnPlateau(monitor='val_accuracy', mode='max', patience=5, factor=0.5, min_lr=1e-5)\n","    ]\n",").history"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xnTUK0hKSCvc"},"outputs":[],"source":["best_epoch = np.argmax(history['val_accuracy'])\n","plt.figure(figsize=(17,4))\n","plt.plot(history['loss'], label='Training loss', alpha=.8, color='#ff7f0e')\n","plt.plot(history['val_loss'], label='Validation loss', alpha=.9, color='#5a9aa5')\n","plt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\n","plt.title('Categorical Crossentropy')\n","plt.legend()\n","plt.grid(alpha=.3)\n","plt.show()\n","\n","plt.figure(figsize=(17,4))\n","plt.plot(history['accuracy'], label='Training accuracy', alpha=.8, color='#ff7f0e')\n","plt.plot(history['val_accuracy'], label='Validation accuracy', alpha=.9, color='#5a9aa5')\n","plt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\n","plt.title('Accuracy')\n","plt.legend()\n","plt.grid(alpha=.3)\n","plt.show()\n","\n","plt.figure(figsize=(18,3))\n","plt.plot(history['lr'], label='Learning Rate', alpha=.8, color='#ff7f0e')\n","plt.axvline(x=best_epoch, label='Best epoch', alpha=.3, ls='--', color='#5a9aa5')\n","plt.legend()\n","plt.grid(alpha=.3)\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vLXqq6Y6SFtc","outputId":"80acf240-af54-41c7-e549-de50c038171d","executionInfo":{"status":"ok","timestamp":1671209094604,"user_tz":-60,"elapsed":32119,"user":{"displayName":"Daniele_Casciani","userId":"18232816629247177598"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:absl:Function `_wrapped_model` contains input name(s) Input with unsupported characters which will be renamed to input in the SavedModel.\n","WARNING:absl:Found untraced functions such as lstm_cell_31_layer_call_fn, lstm_cell_31_layer_call_and_return_conditional_losses, lstm_cell_32_layer_call_fn, lstm_cell_32_layer_call_and_return_conditional_losses, lstm_cell_34_layer_call_fn while saving (showing 5 of 8). These functions will not be directly callable after loading.\n"]}],"source":["model.save('new_df/BiLSTM/ndf_att_roll__batch32_no_norm')\n","# model = tfk.models.load_model('BiLSTM')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HwADVOLaSH2Z"},"outputs":[],"source":["# Predict the test set with the BiLSTM\n","predictions = model.predict(X_test)\n","predictions.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xvzAEmqGSKq6"},"outputs":[],"source":["# Compute the confusion matrix\n","cm = confusion_matrix(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1))\n","\n","# Compute the classification metrics\n","accuracy = accuracy_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1))\n","precision = precision_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')\n","recall = recall_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')\n","f1 = f1_score(np.argmax(y_test, axis=-1), np.argmax(predictions, axis=-1), average='macro')\n","print('Accuracy:',accuracy.round(4))\n","print('Precision:',precision.round(4))\n","print('Recall:',recall.round(4))\n","print('F1:',f1.round(4))\n","\n","# Plot the confusion matrix\n","plt.figure(figsize=(10,8))\n","sns.heatmap(cm.T, cmap='Blues', xticklabels=list([0,1,2,3,4,5,6,7,8,9,10,11]), yticklabels=list([0,1,2,3,4,5,6,7,8,9,10,11]))\n","plt.xlabel('True labels')\n","plt.ylabel('Predicted labels')\n","plt.show()"]}],"metadata":{"colab":{"provenance":[],"collapsed_sections":["2m9NaivlpUHq"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}